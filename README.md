## 📌 概要  
金沢大学 ICNL サーバ上で、近年のトップカンファレンスにおける **LLMベース時系列予測論文** を再現・比較するプロジェクトです。

---

## 🔧 統一設定

- [x] LLM バックボーン: `GPT2-small`
- [x] 精度: `fp16`


## ✅ 今までの進捗
- [x] **OFA** One Fits All:Power General Time Series Analysis by Pretrained LM [[NeurIPS 2023]](https://arxiv.org/abs/2302.11939) [[Code]](https://github.com/DAMO-DI-ML/NeurIPS2023-One-Fits-All.git)

- [x] **Time-LLM** Time-LLM: Time Series Forecasting by Reprogramming Large Language Models [[ICLR 2024]](https://arxiv.org/abs/2310.01728) [[Code]](https://github.com/KimMeen/Time-LLM.git)

- [x] **GPT4MTS** Prompt-based Large Language Model for Multimodal Time-Series Forecasting [[AAAI 2024]](https://ojs.aaai.org/index.php/AAAI/article/view/30383) [[Code]](https://github.com/Flora-jia-jfr/GPT4MTS-Prompt-based-Large-Language-Model-for-Multimodal-Time-series-Forecasting.git)

- [ ] **TimeCMA** TimeCMA: Towards LLM-Empowered Multivariate Time Series Forecasting via Cross-Modality Alignment [[AAAI 2025]](https://arxiv.org/abs/2406.01638) [[Code]](https://github.com/ChenxiLiu-HNU/TimeCMA.git)

- [x] **LLM4TS** LLM4TS: Aligning Pre-Trained LLMs as Data-Efficient Time-Series Forecasters [[ACM TIST 2025]](https://arxiv.org/abs/2308.08469) [[Code]](https://github.com/blacksnail789521/LLM4TS.git)

##  🧪 これから


- [ ] **ChatTime** ChatTime: A Unified Multimodal Time Series Foundation Model Bridging Numerical and Textual Data [[AAAI 2025]](https://arxiv.org/abs/2412.11376) [[Code]](https://github.com/ForestsKing/ChatTime.git)

- [ ] **Timer-XL** Timer-XL: Long-Context Transformers for Unified Time Series Forecasting [[ICLR 2025]](https://arxiv.org/abs/2410.04803) [[Code]](https://github.com/thuml/Timer-XL.git)

- [ ] **Time-MoE** Time-MoE: Billion-Scale Time Series Foundation Models With Mixture Of Experts [[ICLR 2025]](https://arxiv.org/abs/2409.16040) [[Code]](https://github.com/Time-MoE/Time-MoE.git)

- [ ] **ICTSP** In-context Time Series Predictor [[ICLR 2025]](https://arxiv.org/abs/2403.18447) [[Code]](https://github.com/LJC-FVNR/In-context-Time-Series-Predictor.git)

- [ ] **UniTime** UniTime: A Language-Empowered Unified Model for Cross-Domain Time Series Forecasting [[WWW 2024]](https://arxiv.org/abs/2310.09751) [[Code]](https://github.com/liuxu77/UniTime.git)

- [ ] **Chronos** Chronos: Learning the Language of Time Series [[TMLR 2024]](https://arxiv.org/abs/2403.07815) [[Code]](https://github.com/amazon-science/chronos-forecasting.git)

- [ ] **LMTraj** Can Language Beat Numerical Regression? Language-Based Multimodal Trajectory Prediction [[CVPR 2024]](https://arxiv.org/abs/2403.18447) [[Code]](https://github.com/InhwanBae/LMTrajectory.git)

- [ ] **TEST** TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series [[ICLR 2024]](https://arxiv.org/abs/2308.08241) [[Code]](https://github.com/SCXsunchenxi/TEST.git)

- [ ] **S²IP-LLM** S²IP-LLM: Semantic Space Informed Prompt Learning with LLM for Time Series Forecasting [[ICML 2024]](https://arxiv.org/abs/2403.05798) [[Code]](https://github.com/panzijie825/S2IP-LLM.git)

- [ ] **aLLM4TS** Multi-Patch Prediction: Adapting LLMs for Time Series Representation Learning [[ICML 2024]](https://arxiv.org/abs/2402.04852) [[Code]](https://github.com/yxbian23/aLLM4TS.git)

- [ ] **ST-LLM** ST-LLM: Large Language Models Are Effective Temporal Learners [[MDM 2024]](https://arxiv.org/abs/2404.00308) [[Code]](https://github.com/ChenxiLiu-HNU/ST-LLM.git)

- [ ] **Moment** MOMENT: A Family of Open Time-series Foundation Model [[ICML 2024]](https://arxiv.org/abs/2402.03885) [[Code]](https://github.com/moment-timeseries-foundation-model/moment.git)

- [ ] **Timer** Timer: Generative Pre-trained Transformers Are Large Time Series Models [[ICML 2024]](https://arxiv.org/abs/2402.02368) [[Code]](https://github.com/moment-timeseries-foundation-model/moment.git)

- [ ] **Moirai** Unified Training of Universal Time Series Forecasting Transformers [[ICML 2024]](https://arxiv.org/abs/2402.02592) [[Code]](https://github.com/SalesforceAIResearch/uni2ts.git)

- [ ] **TimesFM** A decoder-only foundation model for time-series forecasting [[ICML 2024]](https://arxiv.org/abs/2310.10688) [[Code]](https://github.com/google-research/timesfm.git)

- [ ] **AutoTimes** Autoregressive Time Series Forecasters via Large Language Models [[NeurIPS 2024]](https://arxiv.org/abs/2402.02370) [[Code]](https://github.com/thuml/AutoTimes.git)

- [ ] **TEMPO** TEMPO: Prompt-based Generative Pre-trained Transformer for Time Series Forecasting [[ICLR 2024]](https://arxiv.org/abs/2310.04948) [[Code]](https://github.com/DC-research/TEMPO.git)

- [ ] **AutoTimes** AutoTimes: Autoregressive Time Series Forecasters via Large Language Models [[NeurIPS 2024]](https://arxiv.org/abs/2402.02370) [[Code]](https://github.com/thuml/AutoTimes.git)

- [ ] **TimeGPT-1** TimeGPT-1 [[Nixtla 2024]](https://arxiv.org/abs/2310.03589) [[Code]](https://github.com/Nixtla/nixtla.git)

- [ ] **LLMTime** Large Language Models Are Zero-Shot Time Series Forecasters [[NeurIPS 2023]](https://arxiv.org/abs/2310.07820) [[Code]](https://github.com/ngruver/llmtime.git)

- [ ] **Lag-Llama** Lag-Llama: Towards Foundation Models for Probabilistic Time Series Forecasting [[NeurIPS 2023 Workshop]](https://ar5iv.labs.arxiv.org/html/2310.08278) [[Code]](https://github.com/time-series-foundation-models/lag-llama.git)

## Tips
1. **Accelerate の初期設定**  
   以下の質問には矢印キーで選択し、Enter キーで確定してください。  
   特に最後の混合精度では **`bf16`** を選択してください。

   ```
   In which compute environment are you running?
   ➔ This machine
     AWS (Amazon SageMaker)

   Which type of machine are you using?
     No distributed training
     multi-CPU
   ➔ multi-GPU
     TPU
     MPS

   How many different machines will you use (use more than 1 for multi-node training)? [1]: 1

   Do you wish to optimize your script with torch dynamo? [yes/NO]: NO
   Do you want to use DeepSpeed? [yes/NO]: NO
   Do you want to use FullyShardedDataParallel? [yes/NO]: NO
   Do you want to use Megatron-LM? [yes/NO]: NO

   How many GPU(s) should be used for distributed training? [1]: 2
   What GPU(s) (by id) should be used for training on this machine as a comma-separated list? [all]: 0,1

   Do you wish to use FP16 or BF16 (mixed precision)?
     no
     fp16
   ➔ bf16

   Accelerate configuration saved at ~/.cache/huggingface/accelerate/default_config.yaml
   ```

